{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "match=re.search(\"is\",\"Pawan is learning ML which is a good thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(6, 8), match='is'>\n"
     ]
    }
   ],
   "source": [
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_pattern(text, pattern):\n",
    "    if re.search(pattern, text):\n",
    "        return re.search(pattern,text).group()\n",
    "    else:\n",
    "        return \"not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"*\" means Zero or more\n",
    "# \"?\" means zero or one\n",
    "# \"+\" means one or more\n",
    "# \"{m,n}\" means matchs atleast m times and atmost n times\n",
    "#Anchors\n",
    "# \"^\" starting of the string\n",
    "# \"$\" end of the string\n",
    "# wildcard\n",
    "# \".\" any charachter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta sequences\n",
    "\n",
    "| Pattern  | Equivalent to    |\n",
    "|----------|------------------|\n",
    "| \\s       | [ \\t\\n\\r\\f\\v]    |\n",
    "| \\S       | [^ \\t\\n\\r\\f\\v]   |\n",
    "| \\d       | [0-9]            |\n",
    "| \\D       | [^0-9]           |\n",
    "| \\w       | [a-zA-Z0-9_]     |\n",
    "| \\W       | [^a-zA-Z0-9_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(str_pattern(\"1\", \"\\d\")) #^ neither of these theree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match will look in start of the staring else return not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finall() Find all the substrings where the RE matches, and return them as a list\n",
    "\n",
    "finditer() Find all substrings where RE matches and return them as asn iterator\n",
    "\n",
    "sub() Find all substrings where the RE matches and substitute them with the given string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise, the Zipf's law (discovered by the linguist-statistician George Zipf) states\n",
    "that the frequency of a word is inversely proportional to the rank of the word, where rank 1 is given to the most frequent \n",
    "word, 2 to the second most frequent and so on.This is also called the power law distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pawasharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pawasharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pawasharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Need to create a universal summarization of text which IS very innovative project for us\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['need', 'to', 'create', 'a', 'universal', 'summarization', 'of', 'text', 'which', 'is', 'very', 'innovative', 'project', 'for', 'us']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['need', 'create', 'universal', 'summarization', 'text', 'innovative', 'project', 'us']\n"
     ]
    }
   ],
   "source": [
    "sample_text = [word for word in words if word not in stopwords.words('english')]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is called tokenisation - a technique that‚Äôs used to split the text into smaller elements. \n",
    "These elements can be characters, words, sentences, or even paragraphs depending on the application you‚Äôre working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\n"
     ]
    }
   ],
   "source": [
    "#word tokenization\n",
    "document = \"At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself', '.', 'It', 'looks', 'like', 'religious', 'mania', ',', 'and', 'he', \"'ll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word = word_tokenize(document)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At nine o'clock I visited him myself.\", \"It looks like religious mania, and he'll soon think that he himself is God.\"]\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentence = sent_tokenize(document)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':)', '.', 'i', 'totally', 'loved', 'it', 'üòç', '.', 'it', 'was', 'gr8', '<3', '.', '#bingewatching', '#nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "#tweet tokenization\n",
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\"\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_words = TweetTokenizer()\n",
    "print(tweet_words.tokenize(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#bingewatching', '#nothingtodo']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regex tokenization\n",
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\"\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "pattern = \"#[\\w]+\"\n",
    "regexp_tokenize(message, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gangs of Wasseypur is a great movie.', 'The success of a movie depends on the performance of the actors.', 'There are no new movies releasing this week.']\n"
     ]
    }
   ],
   "source": [
    "documents = [\"Gangs of Wasseypur is a great movie.\", \"The success of a movie depends on the performance of the actors.\", \"There are no new movies releasing this week.\"]\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "    document = document.lower()\n",
    "    document = word_tokenize(document)\n",
    "    words =  [word for word in document if word not in stopwords.words('english')]\n",
    "    document = ' '.join(words)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "document =  [preprocessing(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gangs wasseypur great movie .', 'success movie depends performance actors .', 'new movies releasing week .']\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 0 0 0 0 1 0]\n",
      " [1 1 0 0 1 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "counter_var  = CountVectorizer()\n",
    "bow_model  = counter_var.fit_transform(document)\n",
    "print(bow_model.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming  -- rule bases\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\n"
     ]
    }
   ],
   "source": [
    "text = \"Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer_word = [stemmer.stem(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['veri', 'orderli', 'and', 'method', 'he', 'look', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'tick', 'a', 'sonor', 'sermon', 'under', 'hi', 'flap', 'newli', 'bought', 'waist-coat', ',', 'as', 'though', 'it', 'pit', 'it', 'graviti', 'and', 'longev', 'against', 'the', 'leviti', 'and', 'evanesc', 'of', 'the', 'brisk', 'fire', '.']\n"
     ]
    }
   ],
   "source": [
    "print(stemmer_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization -- not rule based POS tagging is incorrect than go for stemming, pass pos tag of each word with\n",
    "#lemmatiozation for better results\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "lemma_words = [lemma.lemmatize(token) for token in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['very', 'orderly', 'and', 'methodical', 'he', 'looked', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'ticking', 'a', 'sonorous', 'sermon', 'under', 'his', 'flapped', 'newly', 'bought', 'waist-coat', ',', 'a', 'though', 'it', 'pitted', 'it', 'gravity', 'and', 'longevity', 'against', 'the', 'levity', 'and', 'evanescence', 'of', 'the', 'brisk', 'fire', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Model (TF is term freq and IDF is inverse doc freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "def docpreprcess(document):\n",
    "    document = document.lower()\n",
    "    words = word_tokenize(document)\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n",
    "    document = \" \".join(tokens)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vapour, Bangalore has a really great terrace seating and an awesome view of the Bangalore skyline', 'The beer at Vapour, Bangalore was amazing. My favorites are the wheat beer and the ale beer.', 'Vapour, Bangalore has the best view in Bangalore.']\n"
     ]
    }
   ],
   "source": [
    "documents = [\"Vapour, Bangalore has a really great terrace seating and an awesome view of the Bangalore skyline\",\n",
    "             \"The beer at Vapour, Bangalore was amazing. My favorites are the wheat beer and the ale beer.\",\n",
    "             \"Vapour, Bangalore has the best view in Bangalore.\"]\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "document =  [docpreprcess(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vapour , bangalor realli great terrac seat awesom view bangalor skylin', 'beer vapour , bangalor amaz . favorit wheat beer ale beer .', 'vapour , bangalor best view bangalor .']\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidfmodel = tfidf.fit_transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ale</th>\n",
       "      <th>amaz</th>\n",
       "      <th>awesom</th>\n",
       "      <th>bangalor</th>\n",
       "      <th>beer</th>\n",
       "      <th>best</th>\n",
       "      <th>favorit</th>\n",
       "      <th>great</th>\n",
       "      <th>realli</th>\n",
       "      <th>seat</th>\n",
       "      <th>skylin</th>\n",
       "      <th>terrac</th>\n",
       "      <th>vapour</th>\n",
       "      <th>view</th>\n",
       "      <th>wheat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346635</td>\n",
       "      <td>0.409456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346635</td>\n",
       "      <td>0.346635</td>\n",
       "      <td>0.346635</td>\n",
       "      <td>0.346635</td>\n",
       "      <td>0.346635</td>\n",
       "      <td>0.204728</td>\n",
       "      <td>0.263625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.270195</td>\n",
       "      <td>0.270195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159581</td>\n",
       "      <td>0.810584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324019</td>\n",
       "      <td>0.417233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ale      amaz    awesom  bangalor      beer      best   favorit  \\\n",
       "0  0.000000  0.000000  0.346635  0.409456  0.000000  0.000000  0.000000   \n",
       "1  0.270195  0.270195  0.000000  0.159581  0.810584  0.000000  0.270195   \n",
       "2  0.000000  0.000000  0.000000  0.648038  0.000000  0.548612  0.000000   \n",
       "\n",
       "      great    realli      seat    skylin    terrac    vapour      view  \\\n",
       "0  0.346635  0.346635  0.346635  0.346635  0.346635  0.204728  0.263625   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.159581  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.324019  0.417233   \n",
       "\n",
       "      wheat  \n",
       "0  0.000000  \n",
       "1  0.270195  \n",
       "2  0.000000  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidfmodel.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Canonicalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soundex Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit Distince Algo\n",
    "from nltk.metrics.distance import edit_distance\n",
    "edit_distance (\"advise\" , \"advice\", transpositions=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different levels of syntactical analysis:\n",
    "~Part-of-speech tagging\n",
    "    ~Lexicon Tagger(unigram)\n",
    "    ~Rule based Tagger\n",
    "    ~Sochastic tagger\n",
    "    ~Deep learning tagger\n",
    "~Constituency parsing\n",
    "~Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon and Rule based tagger\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "wsj = list(nltk.corpus.treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_word = [tup for sent in wsj for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100676\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "train, test = train_test_split(wsj, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2739\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1175\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rule bases tagger\n",
    "# specify patterns for tagging\n",
    "# example from the NLTK book\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),              # gerund\n",
    "    (r'.*ed$', 'VBD'),               # past tense\n",
    "    (r'.*es$', 'VBZ'),               # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),              # modals\n",
    "    (r'.*\\'s$', 'NN$'),              # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n",
    "    (r'.*', 'NN')                    # nouns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22179675013319125"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90766515716569"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)\n",
    "# lexicon backed up by the rule-based tagger\n",
    "lexicon_tagger = nltk.UnigramTagger(train, backoff=rule_based_tagger)\n",
    "lexicon_tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~Stochastic models such as HMMs Model(Hidden Markov Model)\n",
    "the two main problems in building an HMM for POS tagging  - the learning problem (learning the probabilities) and the explanation problem (solved using the Viterbi algorithm)\n",
    "\n",
    "~Deep-learning, RNN-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing \n",
    "~Constituency Parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context-Free Grammars (CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Down Parsing ( recursive descent parser)-- We should avoid the problem of left-recursion VP -> VP NP| V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification of CFG\n",
    "import nltk \n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N | Det N PP\n",
    "VP -> V | V NP | V NP PP\n",
    "PP -> P NP\n",
    "\n",
    "Det -> 'a' | 'an' | 'the'\n",
    "N -> 'man' | 'park' | 'dog' | 'telescope'\n",
    "V -> 'saw' | 'walked'\n",
    "P -> 'in' | 'with'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"the man saw a dog in the park with a telescope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import RecursiveDescentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a top-down parser\n",
    "rdstr = RecursiveDescentParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (N man))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP\n",
      "      (Det a)\n",
      "      (N dog)\n",
      "      (PP\n",
      "        (P in)\n",
      "        (NP\n",
      "          (Det the)\n",
      "          (N park)\n",
      "          (PP (P with) (NP (Det a) (N telescope))))))))\n",
      "(S\n",
      "  (NP (Det the) (N man))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N dog))\n",
      "    (PP\n",
      "      (P in)\n",
      "      (NP\n",
      "        (Det the)\n",
      "        (N park)\n",
      "        (PP (P with) (NP (Det a) (N telescope)))))))\n",
      "(S\n",
      "  (NP (Det the) (N man))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N dog) (PP (P in) (NP (Det the) (N park))))\n",
      "    (PP (P with) (NP (Det a) (N telescope)))))\n"
     ]
    }
   ],
   "source": [
    "for tree in rdstr.parse(str.split()):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.rdparser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bottom up Parsing\n",
    "str = \"the man saw a dog in the park with a telescope\"\n",
    "from nltk.parse import ShiftReduceParser\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N | Det N PP\n",
    "VP -> V | V NP | V NP PP\n",
    "PP -> P NP\n",
    "\n",
    "Det -> 'a' | 'an' | 'the'\n",
    "N -> 'man' | 'park' | 'dog' | 'telescope'\n",
    "V -> 'saw' | 'walked'\n",
    "P -> 'in' | 'with'\n",
    "\"\"\")\n",
    "shift = ShiftReduceParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in shift.parse(str.split()):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: VP -> V NP PP will never be used\n"
     ]
    }
   ],
   "source": [
    "nltk.app.srparser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
