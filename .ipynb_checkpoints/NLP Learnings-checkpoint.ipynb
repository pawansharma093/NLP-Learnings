{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match=re.search(\"is\",\"Pawan is learning ML which is a good thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_pattern(text, pattern):\n",
    "    if re.search(pattern, text):\n",
    "        return re.search(pattern,text).group()\n",
    "    else:\n",
    "        return \"not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"*\" means Zero or more\n",
    "# \"?\" means zero or one\n",
    "# \"+\" means one or more\n",
    "# \"{m,n}\" means matchs atleast m times and atmost n times\n",
    "#Anchors\n",
    "# \"^\" starting of the string\n",
    "# \"$\" end of the string\n",
    "# wildcard\n",
    "# \".\" any charachter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta sequences\n",
    "\n",
    "| Pattern  | Equivalent to    |\n",
    "|----------|------------------|\n",
    "| \\s       | [ \\t\\n\\r\\f\\v]    |\n",
    "| \\S       | [^ \\t\\n\\r\\f\\v]   |\n",
    "| \\d       | [0-9]            |\n",
    "| \\D       | [^0-9]           |\n",
    "| \\w       | [a-zA-Z0-9_]     |\n",
    "| \\W       | [^a-zA-Z0-9_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str_pattern(\"1\", \"\\d\")) #^ neither of these theree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match will look in start of the staring else return not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finall() Find all the substrings where the RE matches, and return them as a list\n",
    "\n",
    "finditer() Find all substrings where RE matches and return them as asn iterator\n",
    "\n",
    "sub() Find all substrings where the RE matches and substitute them with the given string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise, the Zipf's law (discovered by the linguist-statistician George Zipf) states\n",
    "that the frequency of a word is inversely proportional to the rank of the word, where rank 1 is given to the most frequent \n",
    "word, 2 to the second most frequent and so on.This is also called the power law distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Need to create a universal summarization of text which IS very innovative project for us\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = [word for word in words if word not in stopwords.words('english')]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is called tokenisation - a technique that‚Äôs used to split the text into smaller elements. \n",
    "These elements can be characters, words, sentences, or even paragraphs depending on the application you‚Äôre working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenization\n",
    "document = \"At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word = word_tokenize(document)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentence = sent_tokenize(document)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet tokenization\n",
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\"\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_words = TweetTokenizer()\n",
    "print(tweet_words.tokenize(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex tokenization\n",
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\"\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "pattern = \"#[\\w]+\"\n",
    "regexp_tokenize(message, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Gangs of Wasseypur is a great movie.\", \"The success of a movie depends on the performance of the actors.\", \"There are no new movies releasing this week.\"]\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "    document = document.lower()\n",
    "    document = word_tokenize(document)\n",
    "    words =  [word for word in document if word not in stopwords.words('english')]\n",
    "    document = ' '.join(words)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document =  [preprocessing(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_var  = CountVectorizer()\n",
    "bow_model  = counter_var.fit_transform(document)\n",
    "print(bow_model.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming  -- rule bases\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer_word = [stemmer.stem(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stemmer_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization -- not rule based POS tagging is incorrect than go for stemming, pass pos tag of each word with\n",
    "#lemmatiozation for better results\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "lemma_words = [lemma.lemmatize(token) for token in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Model (TF is term freq and IDF is inverse doc freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "def docpreprcess(document):\n",
    "    document = document.lower()\n",
    "    words = word_tokenize(document)\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n",
    "    document = \" \".join(tokens)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Vapour, Bangalore has a really great terrace seating and an awesome view of the Bangalore skyline\",\n",
    "             \"The beer at Vapour, Bangalore was amazing. My favorites are the wheat beer and the ale beer.\",\n",
    "             \"Vapour, Bangalore has the best view in Bangalore.\"]\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document =  [docpreprcess(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidfmodel = tfidf.fit_transform(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidfmodel.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Canonicalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soundex Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit Distince Algo\n",
    "from nltk.metrics.distance import edit_distance\n",
    "edit_distance (\"advise\" , \"advice\", transpositions=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different levels of syntactical analysis:\n",
    "~Part-of-speech tagging\n",
    "    ~Lexicon Tagger(unigram)\n",
    "    ~Rule based Tagger\n",
    "    ~Sochastic tagger\n",
    "    ~Deep learning tagger\n",
    "~Constituency parsing\n",
    "~Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon and Rule based tagger\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "wsj = list(nltk.corpus.treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_word = [tup for sent in wsj for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tagged_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "train, test = train_test_split(wsj, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rule bases tagger\n",
    "# specify patterns for tagging\n",
    "# example from the NLTK book\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),              # gerund\n",
    "    (r'.*ed$', 'VBD'),               # past tense\n",
    "    (r'.*es$', 'VBZ'),               # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),              # modals\n",
    "    (r'.*\\'s$', 'NN$'),              # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n",
    "    (r'.*', 'NN')                    # nouns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)\n",
    "# lexicon backed up by the rule-based tagger\n",
    "lexicon_tagger = nltk.UnigramTagger(train, backoff=rule_based_tagger)\n",
    "lexicon_tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~Stochastic models such as HMMs Model(Hidden Markov Model)\n",
    "the two main problems in building an HMM for POS tagging  - the learning problem (learning the probabilities) and the explanation problem (solved using the Viterbi algorithm)\n",
    "\n",
    "~Deep-learning, RNN-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing \n",
    "~Constituency Parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context-Free Grammars (CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Down Parsing ( recursive descent parser)-- We should avoid the problem of left-recursion VP -> VP NP| V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specification of CFG\n",
    "import nltk \n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N | Det N PP\n",
    "VP -> V | V NP | V NP PP\n",
    "PP -> P NP\n",
    "\n",
    "Det -> 'a' | 'an' | 'the'\n",
    "N -> 'man' | 'park' | 'dog' | 'telescope'\n",
    "V -> 'saw' | 'walked'\n",
    "P -> 'in' | 'with'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"the man saw a dog in the park with a telescope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import RecursiveDescentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a top-down parser\n",
    "rdstr = RecursiveDescentParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in rdstr.parse(str.split()):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.rdparser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bottom up Parsing\n",
    "str = \"the man saw a dog in the park with a telescope\"\n",
    "from nltk.parse import ShiftReduceParser\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N | Det N PP\n",
    "VP -> V | V NP | V NP PP\n",
    "PP -> P NP\n",
    "\n",
    "Det -> 'a' | 'an' | 'the'\n",
    "N -> 'man' | 'park' | 'dog' | 'telescope'\n",
    "V -> 'saw' | 'walked'\n",
    "P -> 'in' | 'with'\n",
    "\"\"\")\n",
    "shift = ShiftReduceParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in shift.parse(str.split()):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.srparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two topic leaned 1. PCFG and 2.Chomsky Normal Form(CNF) and little bit about Dependency parser(it is used for Free word language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise, a generic IE pipeline is as follows:\n",
    "\n",
    "Preprocessing\n",
    "\n",
    "Sentence Tokenization: sequence segmentation of text.\n",
    "\n",
    "Word Tokenization: breaks down sentences into tokens\n",
    "\n",
    "POS tagging - assigning POS tags to the tokens. The POS tags can be helpful in defining what words could form an entity.\n",
    "\n",
    "Entity Recognition\n",
    "\n",
    "Rule-based models\n",
    "\n",
    "Probabilistic models\n",
    "\n",
    "In entity recognition, every token is tagged with an IOB label and then nearby tokens are combined together basis their labels.\n",
    "\n",
    "Relation Recognition is the task of identifying relationships between the named entities. Using entity recognition, we can identify places (pl), organisations (o), persons (p). Relation recognition will find the relation between (pl,o), such that o is located in pl. Or between (o,p), such that p is working in o, etc.\n",
    "Record Linkage refers to the task of linking two or more records that belong to the same entity. For example, Bangalore and Bengaluru refer to the same entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditonal Random Fields needs to be used for NER or POS tagging, its a better approach then above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symentic Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Lesk Algorithm for the unsuperwisend word sense disambiguation. It uses wordnet to find out the sense of confusing word like duck or bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   \n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import word2vec #need to use '1.19.2' version of numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextCorpus  = [\"I like Upgrad\",\n",
    "               \"Upgrad has a good ML program\",\n",
    "               \"Upgrad has good faculty\",\n",
    "               \"Rahim is that good faculty\",\n",
    "               \"I like ML\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'like', 'Upgrad'], ['Upgrad', 'has', 'a', 'good', 'ML', 'program']]\n"
     ]
    }
   ],
   "source": [
    "text_tokens = [sent.split() for sent in TextCorpus]\n",
    "print(text_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(text_tokens,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.2426788e-03,  9.2993546e-03, -1.9766092e-04, -1.9672776e-03,\n",
       "        4.6036290e-03, -4.0953159e-03,  2.7431131e-03,  6.9399667e-03,\n",
       "        6.0654259e-03, -7.5107957e-03,  9.3823504e-03,  4.6718074e-03,\n",
       "        3.9661191e-03, -6.2435055e-03,  8.4599778e-03, -2.1501661e-03,\n",
       "        8.8251876e-03, -5.3620026e-03, -8.1294207e-03,  6.8245577e-03,\n",
       "        1.6711927e-03, -2.1985101e-03,  9.5135998e-03,  9.4938539e-03,\n",
       "       -9.7740479e-03,  2.5052286e-03,  6.1566923e-03,  3.8724565e-03,\n",
       "        2.0227861e-03,  4.3050051e-04,  6.7363022e-04, -3.8206363e-03,\n",
       "       -7.1402504e-03, -2.0888734e-03,  3.9238976e-03,  8.8186832e-03,\n",
       "        9.2591504e-03, -5.9759379e-03, -9.4026709e-03,  9.7643761e-03,\n",
       "        3.4297847e-03,  5.1661157e-03,  6.2823440e-03, -2.8042626e-03,\n",
       "        7.3227026e-03,  2.8302716e-03,  2.8710032e-03, -2.3803711e-03,\n",
       "       -3.1282497e-03, -2.3701428e-03,  4.2764354e-03,  7.6057913e-05,\n",
       "       -9.5842788e-03, -9.6655441e-03, -6.1481954e-03, -1.2856961e-04,\n",
       "        1.9974159e-03,  9.4319675e-03,  5.5843499e-03, -4.2906976e-03,\n",
       "        2.7831554e-04,  4.9643586e-03,  7.6983096e-03, -1.1442233e-03,\n",
       "        4.3234206e-03, -5.8143805e-03, -8.0419064e-04,  8.1000496e-03,\n",
       "       -2.3600650e-03, -9.6634552e-03,  5.7792594e-03, -3.9298222e-03,\n",
       "       -1.2228728e-03,  9.9805165e-03, -2.2563506e-03, -4.7570658e-03,\n",
       "       -5.3293873e-03,  6.9808890e-03, -5.7088733e-03,  2.1136617e-03,\n",
       "       -5.2556610e-03,  6.1207130e-03,  4.3573068e-03,  2.6063537e-03,\n",
       "       -1.4910841e-03, -2.7460647e-03,  8.9929365e-03,  5.2157734e-03,\n",
       "       -2.1625208e-03, -9.4703101e-03, -7.4260519e-03, -1.0637427e-03,\n",
       "       -7.9494715e-04, -2.5629092e-03,  9.6827196e-03, -4.5852186e-04,\n",
       "        5.8737611e-03, -7.4475883e-03, -2.5060750e-03, -5.5498648e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['ML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv['ML'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 0.13149002194404602),\n",
       " ('is', 0.07497299462556839),\n",
       " ('Upgrad', 0.0679759532213211),\n",
       " ('I', 0.04157735034823418),\n",
       " ('a', 0.04130808264017105)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"ML\",topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.Text8Corpus('C:/Users/pawasharma/Desktop/Python/NLP/text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Text8Corpus at 0x1cc66be3788>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('humanity', 0.7819305658340454),\n",
       " ('pleasure', 0.7682881355285645),\n",
       " ('perfection', 0.7613503336906433),\n",
       " ('compassion', 0.7477684617042542),\n",
       " ('goodness', 0.7453301548957825),\n",
       " ('dignity', 0.7431254386901855),\n",
       " ('fear', 0.7417725324630737),\n",
       " ('mankind', 0.7215771079063416),\n",
       " ('righteousness', 0.7204358577728271),\n",
       " ('desires', 0.7135218381881714)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"happiness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
